---
---
@inproceedings{Li2021How,
  abbr={Neurips},
  title={How Does a Neural Network's Architecture Impact Its Robustness to Noisy Labels?},
  author={Jingling Li and Mozhi Zhang and Keyulu Xu and John Dickerson and Jimmy Ba},
  abstract={Noisy labels are inevitable in large real-world datasets. In this work, we explore an area understudied by previous works --- how the network's architecture impacts its robustness to noisy labels. We provide a formal framework connecting the robustness of a network to the alignments between its architecture and target/noise functions. Our framework measures a network's robustness via the predictive power in its representations --- the test performance of a linear model trained on the learned representations using a small set of clean labels. We hypothesize that a network is more robust to noisy labels if its architecture is more aligned with the target function than the noise. To support our hypothesis, we provide both theoretical and empirical evidence across various neural network architectures and different domains. We also find that when the network is well-aligned with the target function, its predictive power in representations could improve upon state-of-the-art (SOTA) noisy-label-training methods in terms of test accuracy and even outperform sophisticated methods that use clean labels.},
  booktitle={Advances in Neural Information Processing Systems,},
  year={2021},
  url={https://openreview.net/forum?id=Ir-WwGboFN-},
  html={https://openreview.net/forum?id=Ir-WwGboFN-},
  code={https://github.com/jinglingli/alignment_noisy_label},
  selected={true}
}

@inproceedings{Ding2021Vq,
  abbr={Neurips},
  title={VQ-GNN: A Universal Framework to Scale up Graph Neural Networks using Vector Quantization},
  author={Mucong* Ding and Kezhi* Kong and Jingling Li and Chen Zhu and John P Dickerson and Furong Huang and Tom Goldstein},
  abstract={Most state-of-the-art Graph Neural Networks (GNNs) can be defined as a form of graph convolution which can be realized by message passing between direct neighbors or beyond. To scale such GNNs to large graphs, various neighbor-, layer-, or subgraph-sampling techniques are proposed to alleviate the "neighbor explosion" problem by considering only a small subset of messages passed to the nodes in a mini-batch. However, sampling-based methods are difficult to apply to GNNs that utilize many-hops-away or global context each layer, show unstable performance for different tasks and datasets, and do not speed up model inference. We propose a principled and fundamentally different approach, VQ-GNN, a universal framework to scale up any convolution-based GNNs using Vector Quantization (VQ) without compromising the performance. In contrast to sampling-based techniques, our approach can effectively preserve all the messages passed to a mini-batch of nodes by learning and updating a small number of quantized reference vectors of global node representations, using VQ within each GNN layer. Our framework avoids the "neighbor explosion" problem of GNNs using quantized representations combined with a low-rank version of the graph convolution matrix. We show that such a compact low-rank version of the gigantic convolution matrix is sufficient both theoretically and experimentally. In company with VQ, we design a novel approximated message passing algorithm and a nontrivial back-propagation rule for our framework. Experiments on various types of GNN backbones demonstrate the scalability and competitive performance of our framework on large-graph node classification and link prediction benchmarks.},
  booktitle={Advances in Neural Information Processing Systems},
  year={2021},
  url={https://openreview.net/forum?id=EO-CQzgcIxd},
  html={https://openreview.net/forum?id=EO-CQzgcIxd},
  code={https://github.com/devnkong/VQ-GNN},
  selected={false}
}

@inproceedings{Xu2021How,
    abbr={ICLR},
    title={How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks},
    author={Keyulu Xu and Mozhi Zhang and Jingling Li and Simon S. Du and Ken-ichi Kawarabayashi and Stefanie Jegelka},
    abstract={We study how neural networks trained by gradient descent extrapolate, i.e., what they learn outside the support of the training distribution. Previous works report mixed empirical results when extrapolating with neural networks: while feedforward neural networks, a.k.a. multilayer perceptrons (MLPs), do not extrapolate well in certain simple tasks, Graph Neural Networks (GNNs) -- structured networks with MLP modules -- have shown some success in more complex tasks. Working towards a theoretical explanation, we identify conditions under which MLPs and GNNs extrapolate well. First, we quantify the observation that ReLU MLPs quickly converge to linear functions along any direction from the origin, which implies that ReLU MLPs do not extrapolate most nonlinear functions. But, they can provably learn a linear target function when the training distribution is sufficiently "diverse". Second, in connection to analyzing the successes and limitations of GNNs, these results suggest a hypothesis for which we provide theoretical and empirical evidence: the success of GNNs in extrapolating algorithmic tasks to new data (e.g., larger graphs or edge weights) relies on encoding task-specific non-linearities in the architecture or features. Our theoretical analysis builds on a connection of over-parameterized networks to the neural tangent kernel. Empirically, our theory holds across different training settings.},
    booktitle={International Conference on Learning Representations (Oral)},
    year={2021},
    url={https://openreview.net/forum?id=UH-cmocLJC},
    html={https://openreview.net/forum?id=UH-cmocLJC},
    code={https://github.com/jinglingli/nn-extrapolate},
    selected={true}
}

@inproceedings{Xu2020What,
    abbr={ICLR},
    title={What Can Neural Networks Reason About?},
    author={Keyulu Xu and Jingling Li and Mozhi Zhang and Simon S. Du and Ken-ichi Kawarabayashi and Stefanie Jegelka},
    abstract={Neural networks have succeeded in many reasoning tasks. Empirically, these tasks require specialized network structures, e.g., Graph Neural Networks (GNNs) perform well on many such tasks, but less structured networks fail. Theoretically, there is limited understanding of why and when a network structure generalizes better than others, although they have equal expressive power. In this paper, we develop a framework to characterize which reasoning tasks a network can learn well, by studying how well its computation structure aligns with the algorithmic structure of the relevant reasoning process. We formally define this algorithmic alignment and derive a sample complexity bound that decreases with better alignment. This framework offers an explanation for the empirical success of popular reasoning models, and suggests their limitations. As an example, we unify seemingly different reasoning tasks, such as intuitive physics, visual question answering, and shortest paths, via the lens of a powerful algorithmic paradigm, dynamic programming (DP). We show that GNNs align with DP and thus are expected to solve these tasks. On several reasoning tasks, our theory is supported by empirical results.},
    booktitle={International Conference on Learning Representations (Spotlight)},
    year={2020},
    url={https://openreview.net/forum?id=rJxbJeHFPS},
    html={https://openreview.net/forum?id=rJxbJeHFPS},
    code={https://github.com/NNReasoning/What-Can-Neural-Networks-Reason-About},
    selected={true}
}

@inproceedings{Li2020Understanding,
    abbr={AISTATS},
    title={Understanding Generalization in Deep Learning via Tensor Methods},
    author={Jingling Li and Yanchao Sun and Jiahao Su and Taiji Suzuki and Furong Huang},
    abstract={Deep neural networks generalize well on unseen data though the number of parameters often far exceeds the number of training examples. Recently proposed complexity measures have provided insights to understanding the generalizability in neural networks from perspectives of PAC-Bayes, robustness, overparametrization, compression and so on. In this work, we advance the understanding of the relations between the network's architecture and its generalizability from the compression perspective. Using tensor analysis, we propose a series of intuitive, data-dependent and easily-measurable properties that tightly characterize the compressibility and generalizability of neural networks; thus, in practice, our generalization bound outperforms the previous compression-based ones, especially for neural networks using tensors as their weight kernels (e.g. CNNs). Moreover, these intuitive measurements provide further insights into designing neural network architectures with properties favorable for better/guaranteed generalizability. Our experimental results demonstrate that through the proposed measurable properties, our generalization error bound matches the trend of the test error well. Our theoretical analysis further provides justifications for the empirical success and limitations of some widely-used tensor-based compression approaches. We also discover the improvements to the compressibility and robustness of current neural networks when incorporating tensor operations via our proposed layer-wise structure.},
    booktitle={International Conference on Artificial Intelligence and Statistics},
    year={2020},
    url={http://proceedings.mlr.press/v108/li20c/li20c.pdf},
    html={http://proceedings.mlr.press/v108/li20c/li20c.pdf},
    selected={true}
}

@article{khuller2019select,
    abbr={TCS},
    title={Select and permute: An improved online framework for scheduling to minimize weighted completion time},
    author={Khuller, Samir* and Li, Jingling* and Sturmfels, Pascal* and Sun, Kevin* and Venkat, Prayaag*},
    journal={Theoretical Computer Science},
    volume={795},
    pages={420--431},
    year={2019},
    publisher={Elsevier},
    url={https://arxiv.org/abs/1704.06677.pdf},
    url={https://arxiv.org/abs/1704.06677.pdf},
    selected={false}
}