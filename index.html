<!DOCTYPE html>
<html>

  <head>
    
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Jingling  Li


</title>
<meta name="description" content="Jingling's personal website
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåà</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
        <!-- Social Icons -->
        <div class="navbar-brand social">
          <a href="mailto:%6A%69%6E%67%6C%69%6E%67@%63%73.%75%6D%64.%65%64%75"><i class="fas fa-envelope"></i></a>

<a href="https://scholar.google.com/citations?user=9TbXgQ0AAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a>


<a href="https://github.com/jinglingli" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a>
<a href="https://www.linkedin.com/in/jingling-li-35241b7b" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a>
<a href="https://twitter.com/jingling_li" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a>











        </div>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">
              about
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                publications
                
              </a>
          </li>
          
          
          
          
          
          
            <div class="toggle-container">
              <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">
     <span class="font-weight-bold">Jingling</span>  Li
    </h1>
     <p class="desc">(Li Êùé [family name] Jingling ‰∫¨Áé≤ [given name])</p>
  </header>

  <article>
    
    <div class="profile float-right">
      
        <img class="img-fluid z-depth-1 rounded" src="/assets/img/prof_pic.jpeg">
      
      
    </div>
    

    <div class="clearfix">
      <p>I am a Ph.D. student in Computer Science at the <a href="http://www.umd.edu" target="_blank" rel="noopener noreferrer">University of Maryland, College Park</a>, where I am fortunate to be advised by <a href="http://jpdickerson.com" target="_blank" rel="noopener noreferrer">Prof. John Dickerson</a>. 
Before that, I obtained my Bachelor‚Äôs degree in Computer Science and Mathematics from <a href="http://www.brynmawr.edu" target="_blank" rel="noopener noreferrer">Bryn Mawr College</a>.
My research focuses on understanding and enriching the reasoning capabilities of current deep learning models. I believe having the ability to reason is an important and necessary step to achieving general intelligence.</p>

<p>Across my Ph.D., I have interned at <a href="https://www.microsoft.com/en-us/research/about-microsoft-research/" target="_blank" rel="noopener noreferrer">Microsoft Research</a> under the guidance of <a href="https://www.microsoft.com/en-us/research/people/adswamin/" target="_blank" rel="noopener noreferrer">Adith Swaminathan</a> on designing better hindsight learning schemes for combinatorial optimization problems. I have also interned at <a href="http://deepmind.com" target="_blank" rel="noopener noreferrer">DeepMind</a> under the supervision of <a href="https://petar-v.com" target="_blank" rel="noopener noreferrer">Dr. Petar Veliƒçkoviƒá</a>, working on how to re-use the learned knowledge and skills in reinforcement learning. In addition, I have worked closely with <a href="http://jimmylba.github.io" target="_blank" rel="noopener noreferrer">Prof. Jimmy Ba</a> and <a href="http://ibis.t.u-tokyo.ac.jp/suzuki" target="_blank" rel="noopener noreferrer">Prof. Taiji Suziki</a> when doing research internships at <a href="http://vectorinstitute.ai" target="_blank" rel="noopener noreferrer">Vector Institute</a> and <a href="http://aip.riken.jp" target="_blank" rel="noopener noreferrer">RIKEN AIP</a>. 
During my research, I also received great guidance from <a href="http://www.cs.umd.edu/users/perlis" target="_blank" rel="noopener noreferrer">Prof. Don Perlis</a>, <a href="http://furong-huang.com" target="_blank" rel="noopener noreferrer">Prof. Furong Huang</a>, and <a href="http://www.umiacs.umd.edu/people/jbrody" target="_blank" rel="noopener noreferrer">Prof. Justin Brody</a>.</p>


    </div>

    
      <div class="news">
  <h2>news</h2>
  
    <div class="table-responsive">
      <table class="table table-sm table-borderless">
      
      
        <tr>
          <th scope="row">Aug 26, 2022</th>
          <td>
            
              I finished my summer internship at Microsoft Research, and I am continuing working on designing better RL algorithms for combintorial optimization problems.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Jan 21, 2022</th>
          <td>
            
              I passed my preliminary exam, and I am now a Ph.D. candidate!

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Sep 28, 2021</th>
          <td>
            
              Our work on <a href="https://openreview.net/forum?id=Ir-WwGboFN-" target="_blank" rel="noopener noreferrer">How Does a Neural Network‚Äôs Architecture Impact its Robustness to Noisy Labels</a> has been accepted to NeurIPS 2021. <img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20">

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Sep 28, 2021</th>
          <td>
            
              Our work on <a href="https://openreview.net/forum?id=EO-CQzgcIxd" target="_blank" rel="noopener noreferrer">VQ-GNN: A Universal Framework to Scale up Graph Neural Networks using Vector Quantization</a> has been accepted to NeurIPS 2021. <img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20">

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">May 24, 2021</th>
          <td>
            
              Started internship at <a href="http://deepmind.com" target="_blank" rel="noopener noreferrer">DeepMind</a>. <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20">

            
          </td>
        </tr>
      
      </table>
    </div>
  
</div>

    

    
      <div class="publications">
  <h2>selected publications</h2>
  <ol class="bibliography">
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">Neurips</abbr>
    
  
  </div>

  <div id="Li2021How" class="col-sm-8">
    
      <div class="title">How Does a Neural Network‚Äôs Architecture Impact Its Robustness to Noisy Labels?</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Li, Jingling</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Zhang, Mozhi,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Xu, Keyulu,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Dickerson, John,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Ba, Jimmy
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Advances in Neural Information Processing Systems,</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      <a href="https://openreview.net/forum?id=Ir-WwGboFN-" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
    
    
    
    
    
      <a href="https://github.com/jinglingli/alignment_noisy_label" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Noisy labels are inevitable in large real-world datasets. In this work, we explore an area understudied by previous works ‚Äî how the network‚Äôs architecture impacts its robustness to noisy labels. We provide a formal framework connecting the robustness of a network to the alignments between its architecture and target/noise functions. Our framework measures a network‚Äôs robustness via the predictive power in its representations ‚Äî the test performance of a linear model trained on the learned representations using a small set of clean labels. We hypothesize that a network is more robust to noisy labels if its architecture is more aligned with the target function than the noise. To support our hypothesis, we provide both theoretical and empirical evidence across various neural network architectures and different domains. We also find that when the network is well-aligned with the target function, its predictive power in representations could improve upon state-of-the-art (SOTA) noisy-label-training methods in terms of test accuracy and even outperform sophisticated methods that use clean labels.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICLR</abbr>
    
  
  </div>

  <div id="Xu2021How" class="col-sm-8">
    
      <div class="title">How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Xu, Keyulu,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Zhang, Mozhi,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Li, Jingling</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Du, Simon S.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Kawarabayashi, Ken-ichi,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Jegelka, Stefanie
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In International Conference on Learning Representations (Oral)</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      <a href="https://openreview.net/forum?id=UH-cmocLJC" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
    
    
    
    
    
      <a href="https://github.com/jinglingli/nn-extrapolate" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We study how neural networks trained by gradient descent extrapolate, i.e., what they learn outside the support of the training distribution. Previous works report mixed empirical results when extrapolating with neural networks: while feedforward neural networks, a.k.a. multilayer perceptrons (MLPs), do not extrapolate well in certain simple tasks, Graph Neural Networks (GNNs) ‚Äì structured networks with MLP modules ‚Äì have shown some success in more complex tasks. Working towards a theoretical explanation, we identify conditions under which MLPs and GNNs extrapolate well. First, we quantify the observation that ReLU MLPs quickly converge to linear functions along any direction from the origin, which implies that ReLU MLPs do not extrapolate most nonlinear functions. But, they can provably learn a linear target function when the training distribution is sufficiently "diverse". Second, in connection to analyzing the successes and limitations of GNNs, these results suggest a hypothesis for which we provide theoretical and empirical evidence: the success of GNNs in extrapolating algorithmic tasks to new data (e.g., larger graphs or edge weights) relies on encoding task-specific non-linearities in the architecture or features. Our theoretical analysis builds on a connection of over-parameterized networks to the neural tangent kernel. Empirically, our theory holds across different training settings.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICLR</abbr>
    
  
  </div>

  <div id="Xu2020What" class="col-sm-8">
    
      <div class="title">What Can Neural Networks Reason About?</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Xu, Keyulu,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Li, Jingling</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Zhang, Mozhi,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Du, Simon S.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Kawarabayashi, Ken-ichi,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Jegelka, Stefanie
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In International Conference on Learning Representations (Spotlight)</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      <a href="https://openreview.net/forum?id=rJxbJeHFPS" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
    
    
    
    
    
      <a href="https://github.com/NNReasoning/What-Can-Neural-Networks-Reason-About" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Neural networks have succeeded in many reasoning tasks. Empirically, these tasks require specialized network structures, e.g., Graph Neural Networks (GNNs) perform well on many such tasks, but less structured networks fail. Theoretically, there is limited understanding of why and when a network structure generalizes better than others, although they have equal expressive power. In this paper, we develop a framework to characterize which reasoning tasks a network can learn well, by studying how well its computation structure aligns with the algorithmic structure of the relevant reasoning process. We formally define this algorithmic alignment and derive a sample complexity bound that decreases with better alignment. This framework offers an explanation for the empirical success of popular reasoning models, and suggests their limitations. As an example, we unify seemingly different reasoning tasks, such as intuitive physics, visual question answering, and shortest paths, via the lens of a powerful algorithmic paradigm, dynamic programming (DP). We show that GNNs align with DP and thus are expected to solve these tasks. On several reasoning tasks, our theory is supported by empirical results.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">AISTATS</abbr>
    
  
  </div>

  <div id="Li2020Understanding" class="col-sm-8">
    
      <div class="title">Understanding Generalization in Deep Learning via Tensor Methods</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Li, Jingling</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Sun, Yanchao,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Su, Jiahao,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Suzuki, Taiji,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Huang, Furong
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In International Conference on Artificial Intelligence and Statistics</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      <a href="http://proceedings.mlr.press/v108/li20c/li20c.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Deep neural networks generalize well on unseen data though the number of parameters often far exceeds the number of training examples. Recently proposed complexity measures have provided insights to understanding the generalizability in neural networks from perspectives of PAC-Bayes, robustness, overparametrization, compression and so on. In this work, we advance the understanding of the relations between the network‚Äôs architecture and its generalizability from the compression perspective. Using tensor analysis, we propose a series of intuitive, data-dependent and easily-measurable properties that tightly characterize the compressibility and generalizability of neural networks; thus, in practice, our generalization bound outperforms the previous compression-based ones, especially for neural networks using tensors as their weight kernels (e.g. CNNs). Moreover, these intuitive measurements provide further insights into designing neural network architectures with properties favorable for better/guaranteed generalizability. Our experimental results demonstrate that through the proposed measurable properties, our generalization error bound matches the trend of the test error well. Our theoretical analysis further provides justifications for the empirical success and limitations of some widely-used tensor-based compression approaches. We also discover the improvements to the compressibility and robustness of current neural networks when incorporating tensor operations via our proposed layer-wise structure.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
</ol>
</div>

    

    
    <div class="social">
      <div class="contact-icons">
        <a href="mailto:%6A%69%6E%67%6C%69%6E%67@%63%73.%75%6D%64.%65%64%75"><i class="fas fa-envelope"></i></a>

<a href="https://scholar.google.com/citations?user=9TbXgQ0AAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a>


<a href="https://github.com/jinglingli" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a>
<a href="https://www.linkedin.com/in/jingling-li-35241b7b" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a>
<a href="https://twitter.com/jingling_li" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a>











      </div>
      <div class="contact-note">Please feel free to contact me for collaboration.
</div>
    </div>
    
  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    ¬© Copyright 2022 Jingling  Li.
    Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.

    
    
    Last updated: November 22, 2022.
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
